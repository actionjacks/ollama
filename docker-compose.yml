version: '3.4'

services:
    openwebui: #(http://localhost:8333)
        image: ghcr.io/open-webui/open-webui:main
        restart: unless-stopped
        depends_on:
            - ollamadeepseek
        ports:
           - '8333:8080'
        volumes:
          - './open-webui:/app/backend/data'
        environment:
            OLLAMA_BASE_URL: http://ollamadeepseek:11436  # Make sure the address matches the port in ollama deepseek.
        networks:
          - ollama-network

    ollamadeepseek: 
        build:
            context: ./ollama
            dockerfile: Dockerfile
        restart: unless-stopped
        ports:
           - '11436:11436'
        volumes:
          - './ollama:/root/.ollama'  # Folder for storing models.
          - './api:/app/api'  # API Folder.
        environment:
            OLLAMA_HOST: 0.0.0.0  # Host on which Ollama should listen.
            OLLAMA_PORT: 11436  # The port that Ollama listens to internally.
            OLLAMA_MODELS: /root/.ollama/models
        networks:
          - ollama-network
        # If GPU is not available, comment the lines below:
        # (instruction in README.md)
        deploy:
          resources:
            limits:
              memory: 16G
            reservations:
              memory: 14G
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]
        runtime: nvidia
        #

networks:
    ollama-network:
        driver: bridge