version: '3.4'

services:
    openwebui:
        image: ghcr.io/open-webui/open-webui:main
        restart: unless-stopped
        depends_on:
            - ollamadeepseek
        ports:
           - '8333:8080'
        volumes:
          - './open-webui:/app/backend/data'
        environment:
            OLLAMA_BASE_URL: http://ollamadeepseek:11436  # Upewnij się, że adres jest zgodny z portem w ollamadeepseek (http://localhost:8333)

    ollamadeepseek: 
        build:
            context: ./ollama
            dockerfile: Dockerfile
        restart: unless-stopped
        ports:
           - '11436:11436'
        volumes:
          - './ollama:/root/.ollama'  # Folder do przechowywania modeli
          - './api:/app/api'  # Folder z API
        environment:
            OLLAMA_HOST: 0.0.0.0  # Host na którym Ollama ma nasłuchiwać
            OLLAMA_PORT: 11436  # Port, na którym Ollama nasłuchuje wewnętrznie
            OLLAMA_MODELS: /root/.ollama/models  # Folder z modelami, zapewne warto podać pełną ścieżkę
        deploy:
          resources:
            limits:
              memory: 16G  # Zwiększ do 16 GB
            reservations:
              memory: 14G  # Zwiększ rezerwację do 14 GB
              # devices:
              #   - driver: nvidia
              #     count: all
              #     capabilities: [gpu]
